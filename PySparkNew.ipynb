{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in d:\\jupitor\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in d:\\jupitor\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Obtaining dependency information for findspark from https://files.pythonhosted.org/packages/a4/cb/7d2bb508f4ca00a043fd53e8156c11767799d3f534bf451a0942211d5def/findspark-2.0.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
      "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"Python Spark SQL basic example\") \\\n",
    "#     .config(\"spark.jars\",'C:\\\\Users\\\\kiran\\\\Downloads\\\\postgresql-42.2.18.jar') \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "\n",
    "# df = spark.read \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"url\", \"jdbc:postgresql://localhost:5432/metadata\") \\\n",
    "#     .option(\"dbtable\", \"public.customervisits\") \\\n",
    "#     .option(\"user\", 'root') \\\n",
    "#     .option(\"password\",'root') \\\n",
    "#     .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "#     .load()\n",
    "\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.createOrReplaceTempView(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result= spark.sql(\"select rating from test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local\") \\\n",
    "#     .appName(\"Word Count\") \\\n",
    "#     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=spark.read.csv(\"datasets/customervisits.csv\", header=True)\n",
    "# df1=spark.read.csv(\"C:\\\\Users\\\\kiran\\\\OneDrive\\\\Desktop\\\\staff.csv\", header=True)\n",
    "# df2=spark.read.csv(\"C:\\\\Users\\\\kiran\\\\OneDrive\\\\Desktop\\\\customer.csv\", header=True)\n",
    "# df3=spark.read.csv(\"C:\\\\Users\\\\kiran\\\\OneDrive\\\\Desktop\\\\campegin.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df4=df.join(df2,'custid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=spark.read.load(\"C:\\\\Users\\\\kiran\\\\OneDrive\\\\Desktop\\\\customervisits.csv\", format=\"csv\", sep=\";\", inferSchema=True, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.sql(\"SELECT * FROM csv.`C:\\\\Users\\\\kiran\\\\OneDrive\\\\Desktop\\\\customervisits.csv`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\juypitor\\lib\\site-packages\\pyspark\\sql\\context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sqlcontext=SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empdf=sqlcontext.read.csv(\"C:\\\\Users\\\\kiran\\\\OneDrive\\\\Desktop\\\\customervisits.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empdf.select('dish_cuisine').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empdf.filter(empdf['dish_cuisine']=='indian').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empdf.groupBy('dish_cuisine').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empdf.groupBy('indian').agg({qty:7,rating:4.0}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empdf.registerTempTable('employee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sqlcontext.sql('select * from employee').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emppands=empdf.toPandas()\n",
    "# for index,row in emppands.iterrows():\n",
    "#     print(row['dish_cuisine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# import pandas as pd\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local[5]\") \\\n",
    "#     .appName(\"Word Count\") \\\n",
    "#     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.sql(\"SELECT from_utc_timestamp('2016-08-31', 'Asia/Seoul')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+\n",
      "|from_utc_timestamp(2016-08-31, Asia/Seoul)|\n",
      "+------------------------------------------+\n",
      "|                       2016-08-31 09:00:00|\n",
      "+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sno: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- State/UnionTerritory: string (nullable = true)\n",
      " |-- ConfirmedIndianNational: string (nullable = true)\n",
      " |-- ConfirmedForeignNational: string (nullable = true)\n",
      " |-- Cured: string (nullable = true)\n",
      " |-- Deaths: string (nullable = true)\n",
      " |-- Confirmed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(\"datasets/covid.csv\", header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch=StructType([StructField('Sno',StringType(),True),\n",
    "                 StructField('Date',StringType(),True),\n",
    "                StructField('Time',StringType(),True),\n",
    "                StructField('State/UnionTerritory',StringType(),True),\n",
    "                StructField('ConfirmedIndianNational',StringType(),True),\n",
    "                StructField('ConfirmedForeignNational',StringType(),True),\n",
    "                StructField('Cured',StringType(),True),\n",
    "               StructField('Deaths',StringType(),True),\n",
    "               StructField('Confirmed',StringType(),True)])\n",
    "df=spark.read.schema(sch).csv(\"datasets/covid.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+--------------------+-----------------------+------------------------+-----+------+---------+\n",
      "|Sno|    Date|   Time|State/UnionTerritory|ConfirmedIndianNational|ConfirmedForeignNational|Cured|Deaths|Confirmed|\n",
      "+---+--------+-------+--------------------+-----------------------+------------------------+-----+------+---------+\n",
      "|  1|30/01/20|6:00 PM|              Kerala|                      1|                       0|    0|     0|        1|\n",
      "|  2|31/01/20|6:00 PM|              Kerala|                      1|                       0|    0|     0|        1|\n",
      "|  3|01/02/20|6:00 PM|              Kerala|                      2|                       0|    0|     0|        2|\n",
      "|  4|02/02/20|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "|  5|03/02/20|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "|  6|04/02/20|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "|  7|05/02/20|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "|  8|06/02/20|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "|  9|07/02/20|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 10|08/02/20|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 11|09/02/20|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 12|10/02/20|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 13|11/02/20|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 14|12/02/20|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 15|13/02/20|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 16|14/02/20|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 17|15/02/20|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 18|16/02/20|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 19|17/02/20|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 20|18/02/20|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "+---+--------+-------+--------------------+-----------------------+------------------------+-----+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sno: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- State/UnionTerritory: string (nullable = true)\n",
      " |-- ConfirmedIndianNational: string (nullable = true)\n",
      " |-- ConfirmedForeignNational: string (nullable = true)\n",
      " |-- Cured: string (nullable = true)\n",
      " |-- Deaths: string (nullable = true)\n",
      " |-- Confirmed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+--------------------+-----------------------+------------------------+-----+------+---------+\n",
      "|Sno|    Date|   Time|State/UnionTerritory|ConfirmedIndianNational|ConfirmedForeignNational|Cured|Deaths|Confirmed|\n",
      "+---+--------+-------+--------------------+-----------------------+------------------------+-----+------+---------+\n",
      "|  1|30/01/20|6:00 PM|       Postive State|                      1|                       0|    0|     0|        1|\n",
      "|  2|31/01/20|6:00 PM|       Postive State|                      1|                       0|    0|     0|        1|\n",
      "|  3|01/02/20|6:00 PM|       Postive State|                      2|                       0|    0|     0|        2|\n",
      "|  4|02/02/20|6:00 PM|       Postive State|                      3|                       0|    0|     0|        3|\n",
      "|  5|03/02/20|6:00 PM|       Postive State|                      3|                       0|    0|     0|        3|\n",
      "|  6|04/02/20|6:00 PM|       Postive State|                      3|                       0|    0|     0|        3|\n",
      "|  7|05/02/20|6:00 PM|       Postive State|                      3|                       0|    0|     0|        3|\n",
      "|  8|06/02/20|6:00 PM|       Postive State|                      3|                       0|    0|     0|        3|\n",
      "|  9|07/02/20|6:00 PM|       Postive State|                      3|                       0|    0|     0|        3|\n",
      "| 10|08/02/20|6:00 PM|       Postive State|                      3|                       0|    0|     0|        3|\n",
      "| 11|09/02/20|6:00 PM|       Postive State|                      3|                       0|    0|     0|        3|\n",
      "| 12|10/02/20|6:00 PM|       Postive State|                      3|                       0|    0|     0|        3|\n",
      "| 13|11/02/20|6:00 PM|       Postive State|                      3|                       0|    0|     0|        3|\n",
      "| 14|12/02/20|6:00 PM|       Postive State|                      3|                       0|    0|     0|        3|\n",
      "| 15|13/02/20|6:00 PM|       Postive State|                      3|                       0|    0|     0|        3|\n",
      "| 16|14/02/20|6:00 PM|       Postive State|                      3|                       0|    0|     0|        3|\n",
      "| 17|15/02/20|6:00 PM|       Postive State|                      3|                       0|    0|     0|        3|\n",
      "| 18|16/02/20|6:00 PM|       Postive State|                      3|                       0|    0|     0|        3|\n",
      "| 19|17/02/20|6:00 PM|       Postive State|                      3|                       0|    0|     0|        3|\n",
      "| 20|18/02/20|6:00 PM|       Postive State|                      3|                       0|    0|     0|        3|\n",
      "+---+--------+-------+--------------------+-----------------------+------------------------+-----+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('State/UnionTerritory',when(col('State/UnionTerritory')=='Kerala','Postive State').when(col('State/UnionTerritory')=='Maharashatra','Negative State')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_time=df.withColumn(\"Date\",to_date(\"Date\",\"yy/mm/dd\")) \\\n",
    "     .withColumn(\"Time\",to_timestamp(\"Time\",'HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+--------------------+-----------------------+------------------------+-----+------+---------+\n",
      "|Sno|      Date|Time|State/UnionTerritory|ConfirmedIndianNational|ConfirmedForeignNational|Cured|Deaths|Confirmed|\n",
      "+---+----------+----+--------------------+-----------------------+------------------------+-----+------+---------+\n",
      "|  1|2030-01-20|null|              Kerala|                      1|                       0|    0|     0|        1|\n",
      "|  2|2031-01-20|null|              Kerala|                      1|                       0|    0|     0|        1|\n",
      "|  3|2001-01-20|null|              Kerala|                      2|                       0|    0|     0|        2|\n",
      "|  4|2002-01-20|null|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "|  5|2003-01-20|null|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "|  6|2004-01-20|null|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "|  7|2005-01-20|null|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "|  8|2006-01-20|null|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "|  9|2007-01-20|null|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 10|2008-01-20|null|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 11|2009-01-20|null|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 12|2010-01-20|null|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 13|2011-01-20|null|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 14|2012-01-20|null|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 15|2013-01-20|null|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 16|2014-01-20|null|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 17|2015-01-20|null|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 18|2016-01-20|null|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 19|2017-01-20|null|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "| 20|2018-01-20|null|              Kerala|                      3|                       0|    0|     0|        3|\n",
      "+---+----------+----+--------------------+-----------------------+------------------------+-----+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date_time.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n",
      "|year|month|day|\n",
      "+----+-----+---+\n",
      "|2030|    1| 20|\n",
      "|2031|    1| 20|\n",
      "|2001|    1| 20|\n",
      "|2002|    1| 20|\n",
      "|2003|    1| 20|\n",
      "|2004|    1| 20|\n",
      "|2005|    1| 20|\n",
      "|2006|    1| 20|\n",
      "|2007|    1| 20|\n",
      "|2008|    1| 20|\n",
      "|2009|    1| 20|\n",
      "|2010|    1| 20|\n",
      "|2011|    1| 20|\n",
      "|2012|    1| 20|\n",
      "|2013|    1| 20|\n",
      "|2014|    1| 20|\n",
      "|2015|    1| 20|\n",
      "|2016|    1| 20|\n",
      "|2017|    1| 20|\n",
      "|2018|    1| 20|\n",
      "+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "\n",
    "# newdf = elevDF.select(year(elevDF.date).alias('dt_year'), month(elevDF.date).alias('dt_month'), dayofmonth(elevDF.date).alias('dt_day'), dayofyear(elevDF.date).alias('dt_dayofy'), hour(elevDF.date).alias('dt_hour'), minute(elevDF.date).alias('dt_min'), weekofyear(elevDF.date).alias('dt_week_no'), unix_timestamp(elevDF.date).alias('dt_int'))\n",
    "\n",
    "newdf = df_date_time.select(year(df_date_time.Date).alias('year'),month(df_date_time.Date).alias('month'),dayofmonth(df_date_time.Date).alias('day'))\n",
    "newdf.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.option(\"multiline\",\"true\").json('datasets/data.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- @context: string (nullable = true)\n",
      " |-- @id: string (nullable = true)\n",
      " |-- @type: string (nullable = true)\n",
      " |-- conformsTo: string (nullable = true)\n",
      " |-- dataset: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- @type: string (nullable = true)\n",
      " |    |    |-- accessLevel: string (nullable = true)\n",
      " |    |    |-- accrualPeriodicity: string (nullable = true)\n",
      " |    |    |-- bureauCode: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- contactPoint: struct (nullable = true)\n",
      " |    |    |    |-- fn: string (nullable = true)\n",
      " |    |    |    |-- hasEmail: string (nullable = true)\n",
      " |    |    |-- describedBy: string (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- distribution: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- @type: string (nullable = true)\n",
      " |    |    |    |    |-- accessURL: string (nullable = true)\n",
      " |    |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |    |-- downloadURL: string (nullable = true)\n",
      " |    |    |    |    |-- format: string (nullable = true)\n",
      " |    |    |    |    |-- mediaType: string (nullable = true)\n",
      " |    |    |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- identifier: string (nullable = true)\n",
      " |    |    |-- issued: string (nullable = true)\n",
      " |    |    |-- keyword: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- landingPage: string (nullable = true)\n",
      " |    |    |-- modified: string (nullable = true)\n",
      " |    |    |-- programCode: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- publisher: struct (nullable = true)\n",
      " |    |    |    |-- @type: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- spatial: string (nullable = true)\n",
      " |    |    |-- temporal: string (nullable = true)\n",
      " |    |    |-- title: string (nullable = true)\n",
      " |-- describedBy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------+-----------+------------------+----------+--------------------+--------------------+--------------------+\n",
      "|            @context|                 @id|          conformsTo|       @type|accesslevel|accrualPeriodicity|bureauCode|                  fn|               @type|         describedBy|\n",
      "+--------------------+--------------------+--------------------+------------+-----------+------------------+----------+--------------------+--------------------+--------------------+\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|Marine Scotland S...| [dcat:Distribution]|https://project-o...|\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|Marine Scotland S...|[dcat:Distributio...|https://project-o...|\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|Marine Scotland S...|[dcat:Distributio...|https://project-o...|\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|Marine Scotland S...| [dcat:Distribution]|https://project-o...|\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|Marine Scotland S...| [dcat:Distribution]|https://project-o...|\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|Marine Scotland S...| [dcat:Distribution]|https://project-o...|\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|Marine Scotland S...| [dcat:Distribution]|https://project-o...|\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|Marine Scotland S...| [dcat:Distribution]|https://project-o...|\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|Marine Scotland S...| [dcat:Distribution]|https://project-o...|\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|Marine Scotland S...|[dcat:Distributio...|https://project-o...|\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|     Marine Scotland|[dcat:Distributio...|https://project-o...|\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|Marine Scotland S...|[dcat:Distributio...|https://project-o...|\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|     Marine Scotland|[dcat:Distributio...|https://project-o...|\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|               admin| [dcat:Distribution]|https://project-o...|\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|     Marine Scotland|[dcat:Distributio...|https://project-o...|\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|     Marine Scotland|[dcat:Distributio...|https://project-o...|\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|     Marine Scotland|[dcat:Distributio...|https://project-o...|\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|     Marine Scotland| [dcat:Distribution]|https://project-o...|\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|     Marine Scotland| [dcat:Distribution]|https://project-o...|\n",
      "|https://project-o...|http://data.marin...|https://project-o...|dcat:Dataset|     public|              null|  [000:00]|    Marine Scotland | [dcat:Distribution]|https://project-o...|\n",
      "+--------------------+--------------------+--------------------+------------+-----------+------------------+----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#updatedDF = df.withColumn(\"dataset\",explode(\"dataset\")).drop(\"@context\",\"@id\",\"@type\",\"conformsTo\",\"describedBy\")\n",
    "#df.select(col(\"dataset\"),col(\"dataset.@type\"),col(\"dataset.accessLevel\")).show()\n",
    "#df.select(\"dataset\",\"dataset.@type\",\"dataset.accessLevel\").show()\n",
    "updatedDF = df.withColumn(\"dataset\",explode(col(\"dataset\"))).select(\"@context\",\"@id\",\"conformsTo\",\"dataset.@type\",\"dataset.accesslevel\",\"dataset.accrualPeriodicity\",\"dataset.bureauCode\",\"dataset.contactPoint.fn\",\"dataset.distribution.@type\",\"describedBy\")\n",
    "updatedDF.show()\n",
    "\n",
    "df_sel=df_emp_dep.select('job_name','salary').groupby('job_name').agg(avg('salary').alias('Salary'))\n",
    "\n",
    "df_emp_dep=df_emp.join(df_dep,df_emp.dep_id==df_dep.dep_id,\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark.sql.functions as F\n",
    "\n",
    "# result = df.groupBy(\"department\") \\\n",
    "#     .agg(F.max(F.struct(\"salary\", \"employee\")).alias(\"max\")) \\\n",
    "#     .selectExpr(\"max.employee\", \"department\", \"max.salary as highest_salary\")\n",
    "\n",
    "# result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "|age|job        |marital |education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|\n",
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "|59 |admin.     |married |secondary|no     |2343   |yes    |no  |unknown|5  |may  |1042    |1       |-1   |0       |unknown |yes    |\n",
      "|56 |admin.     |married |secondary|no     |45     |no     |no  |unknown|5  |may  |1467    |1       |-1   |0       |unknown |yes    |\n",
      "|41 |technician |married |secondary|no     |1270   |yes    |no  |unknown|5  |may  |1389    |1       |-1   |0       |unknown |yes    |\n",
      "|55 |services   |married |secondary|no     |2476   |yes    |no  |unknown|5  |may  |579     |1       |-1   |0       |unknown |yes    |\n",
      "|54 |admin.     |married |tertiary |no     |184    |no     |no  |unknown|5  |may  |673     |2       |-1   |0       |unknown |yes    |\n",
      "|42 |management |single  |tertiary |no     |0      |yes    |yes |unknown|5  |may  |562     |2       |-1   |0       |unknown |yes    |\n",
      "|56 |management |married |tertiary |no     |830    |yes    |yes |unknown|6  |may  |1201    |1       |-1   |0       |unknown |yes    |\n",
      "|60 |retired    |divorced|secondary|no     |545    |yes    |no  |unknown|6  |may  |1030    |1       |-1   |0       |unknown |yes    |\n",
      "|37 |technician |married |secondary|no     |1      |yes    |no  |unknown|6  |may  |608     |1       |-1   |0       |unknown |yes    |\n",
      "|28 |services   |single  |secondary|no     |5090   |yes    |no  |unknown|6  |may  |1297    |3       |-1   |0       |unknown |yes    |\n",
      "|38 |admin.     |single  |secondary|no     |100    |yes    |no  |unknown|7  |may  |786     |1       |-1   |0       |unknown |yes    |\n",
      "|30 |blue-collar|married |secondary|no     |309    |yes    |no  |unknown|7  |may  |1574    |2       |-1   |0       |unknown |yes    |\n",
      "|29 |management |married |tertiary |no     |199    |yes    |yes |unknown|7  |may  |1689    |4       |-1   |0       |unknown |yes    |\n",
      "|46 |blue-collar|single  |tertiary |no     |460    |yes    |no  |unknown|7  |may  |1102    |2       |-1   |0       |unknown |yes    |\n",
      "|31 |technician |single  |tertiary |no     |703    |yes    |no  |unknown|8  |may  |943     |2       |-1   |0       |unknown |yes    |\n",
      "|35 |management |divorced|tertiary |no     |3837   |yes    |no  |unknown|8  |may  |1084    |1       |-1   |0       |unknown |yes    |\n",
      "|32 |blue-collar|single  |primary  |no     |611    |yes    |no  |unknown|8  |may  |541     |3       |-1   |0       |unknown |yes    |\n",
      "|49 |services   |married |secondary|no     |-8     |yes    |no  |unknown|8  |may  |1119    |1       |-1   |0       |unknown |yes    |\n",
      "|41 |admin.     |married |secondary|no     |55     |yes    |no  |unknown|8  |may  |1120    |2       |-1   |0       |unknown |yes    |\n",
      "|49 |admin.     |divorced|secondary|no     |168    |yes    |yes |unknown|8  |may  |513     |1       |-1   |0       |unknown |yes    |\n",
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv= spark.read.option(\"recursiveFilelookup\",\"True\").options(header='True').csv(\"datasets/bank.csv\")\n",
    "df_csv.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+----------+----------+-------+------+------+---------+\n",
      "|emp_id|emp_name| job_name|manager_id| hire_date| salary|dep_id|dep_id| dep_name|\n",
      "+------+--------+---------+----------+----------+-------+------+------+---------+\n",
      "| 68319| KAYLING|PRESIDENT|          |1991-11-18|6000.00|  1001|  1001|Developer|\n",
      "| 68319| KAYLING|PRESIDENT|          |1991-11-18|6000.00|  1001|  1001|    CIVIL|\n",
      "| 68319| KAYLING|PRESIDENT|          |1991-11-18|6000.00|  1001|  1001|    Admin|\n",
      "| 68319| KAYLING|PRESIDENT|          |1991-11-18|6000.00|  1001|  1001|       IT|\n",
      "| 66928|   BLAZE|  MANAGER|     68319|1991-05-01|2750.00|  3001|  3001|Developer|\n",
      "| 66928|   BLAZE|  MANAGER|     68319|1991-05-01|2750.00|  3001|  3001|    Admin|\n",
      "| 66928|   BLAZE|  MANAGER|     68319|1991-05-01|2750.00|  3001|  3001|       IT|\n",
      "| 66928|   BLAZE|  MANAGER|     68319|1991-05-01|2750.00|  3001|  3001|       IT|\n",
      "| 66928|   BLAZE|  MANAGER|     68319|1991-05-01|2750.00|  3001|  3001|Developer|\n",
      "| 67832|   CLARE|  MANAGER|     68319|1991-06-09|2550.00|  1001|  1001|Developer|\n",
      "| 67832|   CLARE|  MANAGER|     68319|1991-06-09|2550.00|  1001|  1001|    CIVIL|\n",
      "| 67832|   CLARE|  MANAGER|     68319|1991-06-09|2550.00|  1001|  1001|    Admin|\n",
      "| 67832|   CLARE|  MANAGER|     68319|1991-06-09|2550.00|  1001|  1001|       IT|\n",
      "| 65646|   JONAS|  MANAGER|     68319|1991-04-02|2957.00|  2001|  2001|  Network|\n",
      "| 65646|   JONAS|  MANAGER|     68319|1991-04-02|2957.00|  2001|  2001|Developer|\n",
      "| 65646|   JONAS|  MANAGER|     68319|1991-04-02|2957.00|  2001|  2001|  Testing|\n",
      "| 65646|   JONAS|  MANAGER|     68319|1991-04-02|2957.00|  2001|  2001| DataBase|\n",
      "| 65646|   JONAS|  MANAGER|     68319|1991-04-02|2957.00|  2001|  2001|  Network|\n",
      "| 67858| SCARLET|  ANALYST|     65646|1997-04-19|3100.00|  2001|  2001|  Network|\n",
      "| 67858| SCARLET|  ANALYST|     65646|1997-04-19|3100.00|  2001|  2001|Developer|\n",
      "+------+--------+---------+----------+----------+-------+------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp=spark.read.options(header='True').csv('datasets/employee/emp.csv')\n",
    "df_dep=spark.read.options(header='True').csv('datasets/employee/dept.csv')\n",
    "\n",
    "df_emp_dep=df_emp.join(df_dep,df_emp.dep_id==df_dep.dep_id,\"inner\")\n",
    "df_emp_dep.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-------+\n",
      "|emp_id|emp_name| dep_name| salary|\n",
      "+------+--------+---------+-------+\n",
      "| 68319| KAYLING|    Admin|6000.00|\n",
      "| 68319| KAYLING|    CIVIL|6000.00|\n",
      "| 63679|SANDRINE| DataBase| 900.00|\n",
      "| 63679|SANDRINE|Developer| 900.00|\n",
      "| 68319| KAYLING|       IT|6000.00|\n",
      "| 63679|SANDRINE|  Network| 900.00|\n",
      "| 63679|SANDRINE|  Testing| 900.00|\n",
      "+------+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp_dep.createOrReplaceTempView('test')\n",
    "\n",
    "df_ans=spark.sql(\"select emp_id,emp_name,dep_name, salary from \"+\" (select *, row_number() OVER (PARTITION BY dep_name ORDER BY salary DESC) as rn \"+\" FROM test) tmp where rn = 1\")\n",
    "df_ans.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "| job_name|            Salary|\n",
      "+---------+------------------+\n",
      "|  ANALYST|            3100.0|\n",
      "| SALESMAN|            1500.0|\n",
      "|    CLERK|1123.6842105263158|\n",
      "|  MANAGER| 2766.785714285714|\n",
      "|PRESIDENT|            6000.0|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sel=df_emp_dep.select('job_name','salary').groupby('job_name').agg(avg('salary').alias('Salary'))\n",
    "df_sel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "rdd=spark.sparkContext.parallelize((0,20))\n",
    "print(str(rdd.getNumPartitions()))\n",
    "rr=spark.range(0,20)\n",
    "print(str(rr.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "rdd1=rdd.repartition(4)\n",
    "print(str(rdd1.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "rdd2=rdd1.coalesce(2)\n",
    "print(str(rdd2.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"language\",\"users_count\"]\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[language: string, users_count: string]\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize(data)\n",
    "dfFromData2 = spark.createDataFrame(data).toDF(*columns)\n",
    "print(dfFromData2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType \n",
    "schema=StructType([StructField('Course',StringType(),True),\n",
    "                   StructField('fee',StringType(),True)])\n",
    "df=spark.createDataFrame(data=data,schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Course: string (nullable = true)\n",
      " |-- fee: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpd=df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nested struct \n",
    "\n",
    "dataStruct = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3000), \\\n",
    "      ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4000), \\\n",
    "      ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",4000), \\\n",
    "      ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",4000), \\\n",
    "      ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaStruct=StructType([StructField('name',StructType([StructField('FirstName',StringType(),True),\n",
    "                                                      StructField('MiddleName',StringType(),True),\n",
    "                                                      StructField('LastName',StringType(),True)\n",
    "                                                       ])),\n",
    "                                                        StructField('id',StringType(),True),\n",
    "                                                        StructField('gender',StringType(),True),\n",
    "                                                        StructField('salary',IntegerType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=spark.createDataFrame(data=dataStruct,schema=schemaStruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- FirstName: string (nullable = true)\n",
      " |    |-- MiddleName: string (nullable = true)\n",
      " |    |-- LastName: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+--------------------+-----+------+------+\n",
      "|name                |id   |gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|{James, , Smith}    |36636|M     |3000  |\n",
      "|{Michael, Rose, }   |40288|M     |4000  |\n",
      "|{Robert, , Williams}|42114|M     |4000  |\n",
      "|{Maria, Anne, Jones}|39192|F     |4000  |\n",
      "|{Jen, Mary, Brown}  |     |F     |-1    |\n",
      "+--------------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()\n",
    "\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# conf = SparkConf().setAppName(\"ABBC\").setMaster(\"local[*]\")\n",
    "# sc = SparkContext(conf=conf)\n",
    "\n",
    "# rddd=spark.sparkContext.parallelize(range(10))\n",
    "# rddd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1], [2, 3, 4], [5, 6], [7, 8, 9]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddd=spark.sparkContext.parallelize(range(10),4)\n",
    "rddd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 5, 6, 7, 8, 9], [2, 3, 4]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1=rddd.repartition(2)\n",
    "rdd1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2=rddd.coalesce(2)\n",
    "rdd2.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df_emp.repartition(4).withColumn('partition_id',spark_partition_id())\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df_emp.coalesce(2).withColumn('partition_id',spark_partition_id())\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11612\\797471066.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdfa\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"datasets/media.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdfa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime,timedelta\n",
    "dfa=spark.read.csv(\"datasets/media.csv\",header=True)\n",
    "dfa.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11612\\2528957174.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdfa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappid\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'netflix'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappid\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'youtube'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[0mdfa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dfa' is not defined"
     ]
    }
   ],
   "source": [
    "df1=dfa.where((df.appid=='netflix') | (df.appid=='youtube')).select(datetime.now()>=dfa.date(timedelta.days==7))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_week = (datetime.now()- timedelta(days=7))\n",
    "new_df = dfa.where(col(\"date\") >= current_date() - expr(\"INTERVAL 7 days\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "|actid|appid|date|\n",
      "+-----+-----+----+\n",
      "+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+------+\n",
      "|name                |id   |gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|{James, , Smith}    |36636|M     |3100  |\n",
      "|{Michael, Rose, }   |40288|M     |4300  |\n",
      "|{Robert, , Williams}|42114|M     |1400  |\n",
      "|{Maria, Anne, Jones}|39192|F     |5500  |\n",
      "|{Jen, Mary, Brown}  |     |F     |-1    |\n",
      "+--------------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "structureData = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "  ]\n",
    "structureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('id', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "df2 = spark.createDataFrame(data=structureData,schema=structureSchema)\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "update=df2.withColumn(\"otherinfo\",struct(col('id').alias(\"identifierid\"),\n",
    "                                          col('gender').alias(\"gender\"),\n",
    "                                           col('salary').alias(\"salarynew\"),\n",
    "                                        when(col('salary').cast(IntegerType())<2000,\"Low\")\n",
    "                                        .when(col('salary').cast(IntegerType())<4000,\"medium\")\n",
    "                                        )).drop(\"id\",\"gender\",\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------+\n",
      "|name                |otherinfo               |\n",
      "+--------------------+------------------------+\n",
      "|{James, , Smith}    |{36636, M, 3100, medium}|\n",
      "|{Michael, Rose, }   |{40288, M, 4300, null}  |\n",
      "|{Robert, , Williams}|{42114, M, 1400, Low}   |\n",
      "|{Maria, Anne, Jones}|{39192, F, 5500, null}  |\n",
      "|{Jen, Mary, Brown}  |{, F, -1, Low}          |\n",
      "+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "update.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "        ((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n",
    "        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n",
    "        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n",
    "        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n",
    "        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n",
    "        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema=StructType([StructField(\"name\",StructType([StructField(\"FirstName\",StringType(),True),\n",
    "                                     StructField(\"middlename\",StringType(),True),\n",
    "                                     StructField(\"LastName\",StringType(),True)])),\n",
    "                          StructField(\"City\",StringType(),True),\n",
    "                          StructField(\"Gender\",StringType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame(data = data, schema = schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.select(\"name.*\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df2.withColumn(\"new_gender\",when(df2.Gender==\"M\",\"Male\")\n",
    "                                .when(df2.Gender==\"F\",\"Female\")\n",
    "                                .when(df2.Gender.isNull(),\"\")\n",
    "                                 .otherwise(df2.Gender))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.select(df2.columns[:2]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.select(col(\"name.firstname\"),col(\"name.lastname\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.withColumn(\"name.firstname\",lit(5)).withColumn(\"name.lastname\",lit(5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.withColumn(\"Country\", lit(\"USA\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.withColumnRenamed(\"gender\",\"sex\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime,timedelta\n",
    "df=spark.read.csv(\"datasets/media.csv\",header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------------+-----+----------------------------+-----------------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "|City               |Country|Decommisioned|Lat  |Location                    |LocationText           |LocationType  |Long  |RecordNumber|State|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|\n",
      "+-------------------+-------+-------------+-----+----------------------------+-----------------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "|PASEO COSTA DEL SUR|US     |false        |17.96|NA-US-PR-PASEO COSTA DEL SUR|Paseo Costa Del Sur, PR|NOT ACCEPTABLE|-66.22|2           |PR   |NA         |0.38 |-0.87|0.3  |STANDARD   |704    |\n",
      "|BDA SAN LUIS       |US     |false        |18.14|NA-US-PR-BDA SAN LUIS       |Bda San Luis, PR       |NOT ACCEPTABLE|-66.26|10          |PR   |NA         |0.38 |-0.86|0.31 |STANDARD   |709    |\n",
      "|PARC PARQUE        |US     |false        |17.96|NA-US-PR-PARC PARQUE        |Parc Parque, PR        |NOT ACCEPTABLE|-66.22|1           |PR   |NA         |0.38 |-0.87|0.3  |STANDARD   |704    |\n",
      "+-------------------+-------+-------------+-----+----------------------------+-----------------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json=spark.read.json(['datasets/zipcode1.json','datasets/zipcode2.json'])\n",
    "df_json.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "      StructField(\"RecordNumber\",IntegerType(),True),\n",
    "      StructField(\"Zipcode\",IntegerType(),True),\n",
    "      StructField(\"ZipCodeType\",StringType(),True),\n",
    "      StructField(\"City\",StringType(),True),\n",
    "      StructField(\"State\",StringType(),True),\n",
    "      StructField(\"LocationType\",StringType(),True),\n",
    "      StructField(\"Lat\",DoubleType(),True),\n",
    "      StructField(\"Long\",DoubleType(),True),\n",
    "      StructField(\"Xaxis\",IntegerType(),True),\n",
    "      StructField(\"Yaxis\",DoubleType(),True),\n",
    "      StructField(\"Zaxis\",DoubleType(),True),\n",
    "      StructField(\"WorldRegion\",StringType(),True),\n",
    "      StructField(\"Country\",StringType(),True),\n",
    "      StructField(\"LocationText\",StringType(),True),\n",
    "      StructField(\"Location\",StringType(),True),\n",
    "      StructField(\"Decommisioned\",BooleanType(),True),\n",
    "      StructField(\"TaxReturnsFiled\",StringType(),True),\n",
    "      StructField(\"EstimatedPopulation\",IntegerType(),True),\n",
    "      StructField(\"TotalWages\",IntegerType(),True),\n",
    "      StructField(\"Notes\",StringType(),True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_schema = spark.read.schema(schema) \\\n",
    "        .json(\"datasets/zipcodes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|RecordNumber|Zipcode|ZipCodeType|               City|State|  LocationType|  Lat|   Long|Xaxis|Yaxis|Zaxis|WorldRegion|Country|        LocationText|            Location|Decommisioned|TaxReturnsFiled|EstimatedPopulation|TotalWages|        Notes|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|           1|    704|   STANDARD|        PARC PARQUE|   PR|NOT ACCEPTABLE|17.96| -66.22| null|-0.87|  0.3|         NA|     US|     Parc Parque, PR|NA-US-PR-PARC PARQUE|        false|           null|               null|      null|         null|\n",
      "|           2|    704|   STANDARD|PASEO COSTA DEL SUR|   PR|NOT ACCEPTABLE|17.96| -66.22| null|-0.87|  0.3|         NA|     US|Paseo Costa Del S...|NA-US-PR-PASEO CO...|        false|           null|               null|      null|         null|\n",
      "|          10|    709|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| null|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        false|           null|               null|      null|         null|\n",
      "|       61391|  76166|     UNIQUE|  CINGULAR WIRELESS|   TX|NOT ACCEPTABLE|32.72| -97.31| null|-0.83| 0.54|         NA|     US|Cingular Wireless...|NA-US-TX-CINGULAR...|        false|           null|               null|      null|         null|\n",
      "|       61392|  76177|   STANDARD|         FORT WORTH|   TX|       PRIMARY|32.75| -97.33| null|-0.83| 0.54|         NA|     US|      Fort Worth, TX| NA-US-TX-FORT WORTH|        false|           2126|               4053| 122396986|         null|\n",
      "|       61393|  76177|   STANDARD|           FT WORTH|   TX|    ACCEPTABLE|32.75| -97.33| null|-0.83| 0.54|         NA|     US|        Ft Worth, TX|   NA-US-TX-FT WORTH|        false|           2126|               4053| 122396986|         null|\n",
      "|           4|    704|   STANDARD|    URB EUGENE RICE|   PR|NOT ACCEPTABLE|17.96| -66.22| null|-0.87|  0.3|         NA|     US| Urb Eugene Rice, PR|NA-US-PR-URB EUGE...|        false|           null|               null|      null|         null|\n",
      "|       39827|  85209|   STANDARD|               MESA|   AZ|       PRIMARY|33.37|-111.64| null|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        false|          14962|              26883| 563792730|no NWS data, |\n",
      "|       39828|  85210|   STANDARD|               MESA|   AZ|       PRIMARY|33.38|-111.84| null|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        false|          14374|              25446| 471000465|         null|\n",
      "|       49345|  32046|   STANDARD|           HILLIARD|   FL|       PRIMARY|30.69| -81.92| null|-0.85| 0.51|         NA|     US|        Hilliard, FL|   NA-US-FL-HILLIARD|        false|           3922|               7443| 133112149|         null|\n",
      "|       49346|  34445|     PO BOX|             HOLDER|   FL|       PRIMARY|28.96| -82.41| null|-0.86| 0.48|         NA|     US|          Holder, FL|     NA-US-FL-HOLDER|        false|           null|               null|      null|         null|\n",
      "|       49347|  32564|   STANDARD|               HOLT|   FL|       PRIMARY|30.72| -86.67| null|-0.85| 0.51|         NA|     US|            Holt, FL|       NA-US-FL-HOLT|        false|           1207|               2190|  36395913|         null|\n",
      "|       49348|  34487|     PO BOX|          HOMOSASSA|   FL|       PRIMARY|28.78| -82.61| null|-0.86| 0.48|         NA|     US|       Homosassa, FL|  NA-US-FL-HOMOSASSA|        false|           null|               null|      null|         null|\n",
      "|          10|    708|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| null|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        false|           null|               null|      null|         null|\n",
      "|           3|    704|   STANDARD|      SECT LANAUSSE|   PR|NOT ACCEPTABLE|17.96| -66.22| null|-0.87|  0.3|         NA|     US|   Sect Lanausse, PR|NA-US-PR-SECT LAN...|        false|           null|               null|      null|         null|\n",
      "|       54354|  36275|     PO BOX|      SPRING GARDEN|   AL|       PRIMARY|33.97| -85.55| null|-0.82| 0.55|         NA|     US|   Spring Garden, AL|NA-US-AL-SPRING G...|        false|           null|               null|      null|         null|\n",
      "|       54355|  35146|   STANDARD|        SPRINGVILLE|   AL|       PRIMARY|33.77| -86.47| null|-0.82| 0.55|         NA|     US|     Springville, AL|NA-US-AL-SPRINGVILLE|        false|           4046|               7845| 172127599|         null|\n",
      "|       54356|  35585|   STANDARD|        SPRUCE PINE|   AL|       PRIMARY|34.37| -87.69| null|-0.82| 0.56|         NA|     US|     Spruce Pine, AL|NA-US-AL-SPRUCE PINE|        false|            610|               1209|  18525517|         null|\n",
      "|       76511|  27007|   STANDARD|           ASH HILL|   NC|NOT ACCEPTABLE| 36.4| -80.56| null|-0.79| 0.59|         NA|     US|        Ash Hill, NC|   NA-US-NC-ASH HILL|        false|            842|               1666|  28876493|         null|\n",
      "|       76512|  27203|   STANDARD|           ASHEBORO|   NC|       PRIMARY|35.71| -79.81| null|-0.79| 0.58|         NA|     US|        Asheboro, NC|   NA-US-NC-ASHEBORO|        false|           8355|              15228| 215474318|         null|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "|age|         job| marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|\n",
      "+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "| 41| blue-collar|  single|  primary|     no|   1618|    yes|  no|unknown| 14|  may|    1553|       1|   -1|       0| unknown|    yes|\n",
      "| 30|entrepreneur|  single|  primary|     no|      0|    yes| yes|unknown| 27|  may|    1051|       1|   -1|       0| unknown|    yes|\n",
      "| 60|     retired| married|  primary|     no|   1542|    yes|  no|unknown| 14|  may|     930|       1|   -1|       0| unknown|    yes|\n",
      "| 48| blue-collar|divorced|  primary|     no|     24|    yes|  no|unknown| 14|  may|     832|       1|   -1|       0| unknown|    yes|\n",
      "| 52| blue-collar|divorced|  primary|     no|   -191|    yes|  no|unknown| 15|  may|     755|       1|   -1|       0| unknown|    yes|\n",
      "| 60|  technician| married|  primary|     no|     65|    yes|  no|unknown|  9|  may|    1028|       2|   -1|       0| unknown|    yes|\n",
      "| 59| blue-collar| married|  primary|     no|    320|    yes|  no|unknown| 15|  may|     695|       1|   -1|       0| unknown|    yes|\n",
      "| 35| blue-collar| married|  primary|     no|    414|     no|  no|unknown| 13|  may|     504|       4|   -1|       0| unknown|    yes|\n",
      "| 49|     unknown| married|  primary|     no|    341|    yes| yes|unknown| 15|  may|     520|       2|   -1|       0| unknown|    yes|\n",
      "| 60| blue-collar| married|  primary|     no|   1262|    yes| yes|unknown| 13|  may|    1015|       1|   -1|       0| unknown|    yes|\n",
      "| 40|    services| married|  primary|     no|     -9|    yes|  no|unknown| 15|  may|     920|       2|   -1|       0| unknown|    yes|\n",
      "| 53|  technician|divorced|  primary|     no|   1443|    yes|  no|unknown| 14|  may|     476|       1|   -1|       0| unknown|    yes|\n",
      "| 41|      admin.| married|  primary|     no|   -306|    yes|  no|unknown| 15|  may|     500|       1|   -1|       0| unknown|    yes|\n",
      "| 35| blue-collar| married|  primary|     no|    102|    yes|  no|unknown| 20|  may|    1334|       1|   -1|       0| unknown|    yes|\n",
      "| 40| blue-collar| married|  primary|     no|    278|    yes|  no|unknown| 23|  may|    1015|       3|   -1|       0| unknown|    yes|\n",
      "| 36| blue-collar| married|  primary|     no|    408|    yes| yes|unknown| 20|  may|    1063|       2|   -1|       0| unknown|    yes|\n",
      "| 32| blue-collar|  single|  primary|     no|    611|    yes|  no|unknown|  8|  may|     541|       3|   -1|       0| unknown|    yes|\n",
      "| 41| blue-collar| married|  primary|     no|   1250|    yes|  no|unknown| 20|  may|    1392|       2|   -1|       0| unknown|    yes|\n",
      "| 23|entrepreneur|  single|  primary|     no|      4|    yes|  no|unknown| 13|  may|     395|       2|   -1|       0| unknown|    yes|\n",
      "| 31| blue-collar|  single|  primary|     no|    216|    yes|  no|unknown| 21|  may|     565|       1|   -1|       0| unknown|    yes|\n",
      "+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv=df_csv.sort(col('education'))\n",
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+---------+-------+-------+-------+----+---------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "|age|        job| marital|education|default|balance|housing|loan|  contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|\n",
      "+---+-----------+--------+---------+-------+-------+-------+----+---------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "| 68|    retired|divorced|  primary|     no|    695|     no|  no| cellular|  9|  apr|     233|       1|   -1|       0| unknown|    yes|\n",
      "| 41|blue-collar|divorced|  primary|     no|    285|    yes|  no| cellular| 20|  apr|    1272|       2|   -1|       0| unknown|    yes|\n",
      "| 42|blue-collar| married|  primary|     no|   2103|    yes|  no| cellular| 13|  apr|     384|       2|   -1|       0| unknown|    yes|\n",
      "| 21|    student|  single|  primary|     no|    423|     no|  no| cellular|  8|  apr|     104|       5|   -1|       0| unknown|    yes|\n",
      "| 41|  housemaid| married|  primary|     no|      0|     no|  no| cellular| 14|  apr|     168|       1|   -1|       0| unknown|    yes|\n",
      "| 34|blue-collar| married|  primary|     no|    329|    yes|  no| cellular| 17|  apr|     952|       1|  343|       1| failure|    yes|\n",
      "| 73|    retired| married|  primary|     no|    253|     no|  no| cellular| 20|  apr|     345|       1|   -1|       0| unknown|    yes|\n",
      "| 37|blue-collar| married|  primary|     no|   3154|    yes|  no| cellular| 17|  apr|     404|       1|  305|       3| failure|    yes|\n",
      "| 43|   services| married|  primary|     no|   2557|    yes|  no|telephone| 15|  apr|     165|       2|  327|       3| failure|    yes|\n",
      "| 40| technician| married|  primary|     no|    284|    yes|  no| cellular| 20|  apr|    1332|       2|   -1|       0| unknown|    yes|\n",
      "| 57|   services| married|  primary|     no|    491|    yes|  no| cellular| 15|  apr|    1217|       3|   -1|       0| unknown|    yes|\n",
      "| 46| management| married|  primary|     no|   3229|    yes|  no| cellular| 20|  apr|    1550|       1|   -1|       0| unknown|    yes|\n",
      "| 56|blue-collar| married|  primary|     no|   3498|     no|  no| cellular| 15|  apr|     264|       2|   -1|       0| unknown|    yes|\n",
      "| 50|    unknown| married|  primary|     no|    341|    yes| yes| cellular| 20|  apr|     670|       4|  340|       2| success|    yes|\n",
      "| 40|   services|divorced|  primary|     no|    471|    yes|  no| cellular| 16|  apr|    1156|       1|  345|       2| failure|    yes|\n",
      "| 45|blue-collar| married|  primary|    yes|   -443|    yes| yes| cellular| 20|  apr|     691|       1|   -1|       0| unknown|    yes|\n",
      "| 50|blue-collar| married|  primary|     no|   4979|     no|  no|telephone|  6|  apr|     180|       1|   -1|       0| unknown|    yes|\n",
      "| 36|blue-collar| married|  primary|     no|   1925|    yes|  no| cellular| 20|  apr|     904|       2|   -1|       0| unknown|    yes|\n",
      "| 76|  housemaid|divorced|  primary|     no|   1411|     no|  no|telephone| 15|  apr|     170|       3|   -1|       0| unknown|    yes|\n",
      "| 78|    retired| married|  primary|     no|    240|     no|  no| cellular| 20|  apr|     325|       1|   -1|       0| unknown|    yes|\n",
      "+---+-----------+--------+---------+-------+-------+-------+----+---------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv=df_csv.orderBy(col('month').asc(),col('education').asc())\n",
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|          job|sum(duration)|\n",
      "+-------------+-------------+\n",
      "|   management|       927563|\n",
      "|      retired|       303415|\n",
      "|      unknown|        23126|\n",
      "|self-employed|       160443|\n",
      "|      student|       119042|\n",
      "|  blue-collar|       767217|\n",
      "| entrepreneur|       121420|\n",
      "|       admin.|       464138|\n",
      "|   technician|       663192|\n",
      "|     services|       356237|\n",
      "|    housemaid|        95447|\n",
      "|   unemployed|       150955|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv=df_csv.withColumn('duration',col('duration').cast(IntegerType())).groupBy('job').sum('duration')\n",
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column 'duration' does not exist. Did you mean one of the following? [sum_salary.job, sum_salary.min(duration)];\n'Project [job#2306, unresolvedalias(cast('duration as int), None)]\n+- SubqueryAlias sum_salary\n   +- Aggregate [job#2306], [job#2306, min(duration#2436) AS min(duration)#2442]\n      +- Project [job#2306, cast(duration#2316 as int) AS duration#2436]\n         +- Relation [age#2305,job#2306,marital#2307,education#2308,default#2309,balance#2310,housing#2311,loan#2312,contact#2313,day#2314,month#2315,duration#2316,campaign#2317,pdays#2318,previous#2319,poutcome#2320,deposit#2321] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7040\\227333698.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_csv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_csv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'duration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIntegerType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'duration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sum_salary\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_csv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\juypitor\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   2021\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2022\u001b[0m         \"\"\"\n\u001b[1;32m-> 2023\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2024\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\juypitor\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\juypitor\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Column 'duration' does not exist. Did you mean one of the following? [sum_salary.job, sum_salary.min(duration)];\n'Project [job#2306, unresolvedalias(cast('duration as int), None)]\n+- SubqueryAlias sum_salary\n   +- Aggregate [job#2306], [job#2306, min(duration#2436) AS min(duration)#2442]\n      +- Project [job#2306, cast(duration#2316 as int) AS duration#2436]\n         +- Relation [age#2305,job#2306,marital#2307,education#2308,default#2309,balance#2310,housing#2311,loan#2312,contact#2313,day#2314,month#2315,duration#2316,campaign#2317,pdays#2318,previous#2319,poutcome#2320,deposit#2321] csv\n"
     ]
    }
   ],
   "source": [
    "df_csv=df_csv.select(col('job'),col('duration').cast(IntegerType())).groupBy('job').max('duration').alias(\"sum_salary\")\n",
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|          job|min(duration)|\n",
      "+-------------+-------------+\n",
      "|   management|            4|\n",
      "|      retired|            8|\n",
      "|      unknown|            8|\n",
      "|self-employed|            6|\n",
      "|      student|            8|\n",
      "|  blue-collar|            2|\n",
      "| entrepreneur|            7|\n",
      "|       admin.|            7|\n",
      "|   technician|            5|\n",
      "|     services|            7|\n",
      "|    housemaid|           10|\n",
      "|   unemployed|            4|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv=df_csv.select(col('job'),col('duration').cast(IntegerType())).groupBy('job').min('duration')\n",
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|          job|min(duration)|\n",
      "+-------------+-------------+\n",
      "|   management|            4|\n",
      "|      retired|            8|\n",
      "|      unknown|            8|\n",
      "|self-employed|            6|\n",
      "|      student|            8|\n",
      "|  blue-collar|            2|\n",
      "| entrepreneur|            7|\n",
      "|       admin.|            7|\n",
      "|   technician|            5|\n",
      "|     services|            7|\n",
      "|    housemaid|           10|\n",
      "|   unemployed|            4|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv=df_csv.withColumn('duration',col('duration').alias('duration').cast(IntegerType())).groupBy('job').min('duration')\n",
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_union=df_emp.unionAll(df_dep)\n",
    "# df_union.show(truncate=False)\n",
    "\n",
    "# df_union=df_emp.union(df_dep)\n",
    "# df_union.show(truncate=False)\n",
    "\n",
    "# df_union=df_emp.union(df_dep).distinct()\n",
    "# df_union.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+----------+----------+-------+------+---------+\n",
      "|emp_id|emp_name|job_name |manager_id|hire_date |salary |dep_id|dep_name |\n",
      "+------+--------+---------+----------+----------+-------+------+---------+\n",
      "|67832 |CLARE   |MANAGER  |68319     |1991-06-09|2550.00|1001  |null     |\n",
      "|65271 |WADE    |SALESMAN |66928     |1991-02-22|1350.00|3001  |null     |\n",
      "|67858 |SCARLET |ANALYST  |65646     |1997-04-19|3100.00|2001  |null     |\n",
      "|68454 |TUCKER  |SALESMAN |66928     |1991-09-08|1600.00|3001  |null     |\n",
      "|69062 |FRANK   |ANALYST  |65646     |1991-12-03|3100.00|2001  |null     |\n",
      "|69324 |MARKER  |CLERK    |67832     |1992-01-23|1400.00|1001  |null     |\n",
      "|63679 |SANDRINE|CLERK    |69062     |1990-12-18|900.00 |2001  |null     |\n",
      "|66564 |MADDEN  |SALESMAN |66928     |1991-09-28|1350.00|3001  |null     |\n",
      "|68319 |KAYLING |PRESIDENT|          |1991-11-18|6000.00|1001  |null     |\n",
      "|66928 |BLAZE   |MANAGER  |68319     |1991-05-01|2750.00|3001  |null     |\n",
      "|69000 |JULIUS  |CLERK    |66928     |1991-12-03|1050.00|3001  |null     |\n",
      "|64989 |ADELYN  |SALESMAN |66928     |1991-02-20|1700.00|3001  |null     |\n",
      "|65646 |JONAS   |MANAGER  |68319     |1991-04-02|2957.00|2001  |null     |\n",
      "|68736 |ADNRES  |CLERK    |67858     |1997-05-23|1200.00|2001  |null     |\n",
      "|null  |null    |null     |null      |null      |null   |2001  |Developer|\n",
      "|null  |null    |null     |null      |null      |null   |2001  |DataBase |\n",
      "|null  |null    |null     |null      |null      |null   |1001  |Admin    |\n",
      "|null  |null    |null     |null      |null      |null   |2001  |Network  |\n",
      "|null  |null    |null     |null      |null      |null   |1001  |Developer|\n",
      "|null  |null    |null     |null      |null      |null   |1001  |IT       |\n",
      "+------+--------+---------+----------+----------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_union=df_emp.unionByName(df_dep,allowMissingColumns=True).distinct()\n",
    "df_union.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "| job_name| salary|\n",
      "+---------+-------+\n",
      "|PRESIDENT|6000.00|\n",
      "|  ANALYST|3100.00|\n",
      "|  ANALYST|3100.00|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowspec=Window.partitionBy('job_name').orderBy(col('salary').asc())\n",
    "\n",
    "df_emp_row=df_emp.withColumn('row_number',row_number().over(windowspec))\n",
    "a=df_emp_row.select('job_name','salary').filter(df_emp_row.salary>3000)\n",
    "a.show()                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+----------+----------+-------+------+----+\n",
      "|emp_id|emp_name|job_name |manager_id|hire_date |salary |dep_id|rank|\n",
      "+------+--------+---------+----------+----------+-------+------+----+\n",
      "|67858 |SCARLET |ANALYST  |65646     |1997-04-19|3100.00|2001  |1   |\n",
      "|69062 |FRANK   |ANALYST  |65646     |1991-12-03|3100.00|2001  |1   |\n",
      "|69000 |JULIUS  |CLERK    |66928     |1991-12-03|1050.00|3001  |1   |\n",
      "|68736 |ADNRES  |CLERK    |67858     |1997-05-23|1200.00|2001  |2   |\n",
      "|69324 |MARKER  |CLERK    |67832     |1992-01-23|1400.00|1001  |3   |\n",
      "|63679 |SANDRINE|CLERK    |69062     |1990-12-18|900.00 |2001  |4   |\n",
      "|67832 |CLARE   |MANAGER  |68319     |1991-06-09|2550.00|1001  |1   |\n",
      "|66928 |BLAZE   |MANAGER  |68319     |1991-05-01|2750.00|3001  |2   |\n",
      "|65646 |JONAS   |MANAGER  |68319     |1991-04-02|2957.00|2001  |3   |\n",
      "|68319 |KAYLING |PRESIDENT|          |1991-11-18|6000.00|1001  |1   |\n",
      "|65271 |WADE    |SALESMAN |66928     |1991-02-22|1350.00|3001  |1   |\n",
      "|66564 |MADDEN  |SALESMAN |66928     |1991-09-28|1350.00|3001  |1   |\n",
      "|68454 |TUCKER  |SALESMAN |66928     |1991-09-08|1600.00|3001  |3   |\n",
      "|64989 |ADELYN  |SALESMAN |66928     |1991-02-20|1700.00|3001  |4   |\n",
      "+------+--------+---------+----------+----------+-------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp_rank=df_emp.withColumn('rank',rank().over(windowspec))\n",
    "df_emp_rank.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+----------+----------+-------+------+---------+\n",
      "|emp_id|emp_name|job_name |manager_id|hire_date |salary |dep_id|denserank|\n",
      "+------+--------+---------+----------+----------+-------+------+---------+\n",
      "|67858 |SCARLET |ANALYST  |65646     |1997-04-19|3100.00|2001  |1        |\n",
      "|69062 |FRANK   |ANALYST  |65646     |1991-12-03|3100.00|2001  |1        |\n",
      "|69000 |JULIUS  |CLERK    |66928     |1991-12-03|1050.00|3001  |1        |\n",
      "|68736 |ADNRES  |CLERK    |67858     |1997-05-23|1200.00|2001  |2        |\n",
      "|69324 |MARKER  |CLERK    |67832     |1992-01-23|1400.00|1001  |3        |\n",
      "|63679 |SANDRINE|CLERK    |69062     |1990-12-18|900.00 |2001  |4        |\n",
      "|67832 |CLARE   |MANAGER  |68319     |1991-06-09|2550.00|1001  |1        |\n",
      "|66928 |BLAZE   |MANAGER  |68319     |1991-05-01|2750.00|3001  |2        |\n",
      "|65646 |JONAS   |MANAGER  |68319     |1991-04-02|2957.00|2001  |3        |\n",
      "|68319 |KAYLING |PRESIDENT|          |1991-11-18|6000.00|1001  |1        |\n",
      "|65271 |WADE    |SALESMAN |66928     |1991-02-22|1350.00|3001  |1        |\n",
      "|66564 |MADDEN  |SALESMAN |66928     |1991-09-28|1350.00|3001  |1        |\n",
      "|68454 |TUCKER  |SALESMAN |66928     |1991-09-08|1600.00|3001  |2        |\n",
      "|64989 |ADELYN  |SALESMAN |66928     |1991-02-20|1700.00|3001  |3        |\n",
      "+------+--------+---------+----------+----------+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp_denrank=df_emp.withColumn('denserank',dense_rank().over(windowspec))\n",
    "df_emp_denrank.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+----------+----------+-------+------+-------+\n",
      "|emp_id|emp_name|job_name |manager_id|hire_date |salary |dep_id|lag    |\n",
      "+------+--------+---------+----------+----------+-------+------+-------+\n",
      "|67858 |SCARLET |ANALYST  |65646     |1997-04-19|3100.00|2001  |0      |\n",
      "|69062 |FRANK   |ANALYST  |65646     |1991-12-03|3100.00|2001  |3100.00|\n",
      "|69000 |JULIUS  |CLERK    |66928     |1991-12-03|1050.00|3001  |0      |\n",
      "|68736 |ADNRES  |CLERK    |67858     |1997-05-23|1200.00|2001  |1050.00|\n",
      "|69324 |MARKER  |CLERK    |67832     |1992-01-23|1400.00|1001  |1200.00|\n",
      "|63679 |SANDRINE|CLERK    |69062     |1990-12-18|900.00 |2001  |1400.00|\n",
      "|67832 |CLARE   |MANAGER  |68319     |1991-06-09|2550.00|1001  |0      |\n",
      "|66928 |BLAZE   |MANAGER  |68319     |1991-05-01|2750.00|3001  |2550.00|\n",
      "|65646 |JONAS   |MANAGER  |68319     |1991-04-02|2957.00|2001  |2750.00|\n",
      "|68319 |KAYLING |PRESIDENT|          |1991-11-18|6000.00|1001  |0      |\n",
      "|65271 |WADE    |SALESMAN |66928     |1991-02-22|1350.00|3001  |0      |\n",
      "|66564 |MADDEN  |SALESMAN |66928     |1991-09-28|1350.00|3001  |1350.00|\n",
      "|68454 |TUCKER  |SALESMAN |66928     |1991-09-08|1600.00|3001  |1350.00|\n",
      "|64989 |ADELYN  |SALESMAN |66928     |1991-02-20|1700.00|3001  |1600.00|\n",
      "+------+--------+---------+----------+----------+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp_lag=df_emp.withColumn('lag',lag('salary',1,0).over(windowspec))\n",
    "df_emp_lag.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+----------+----------+-------+------+-------+\n",
      "|emp_id|emp_name|job_name |manager_id|hire_date |salary |dep_id|lead   |\n",
      "+------+--------+---------+----------+----------+-------+------+-------+\n",
      "|67858 |SCARLET |ANALYST  |65646     |1997-04-19|3100.00|2001  |3100.00|\n",
      "|69062 |FRANK   |ANALYST  |65646     |1991-12-03|3100.00|2001  |0      |\n",
      "|69000 |JULIUS  |CLERK    |66928     |1991-12-03|1050.00|3001  |1200.00|\n",
      "|68736 |ADNRES  |CLERK    |67858     |1997-05-23|1200.00|2001  |1400.00|\n",
      "|69324 |MARKER  |CLERK    |67832     |1992-01-23|1400.00|1001  |900.00 |\n",
      "|63679 |SANDRINE|CLERK    |69062     |1990-12-18|900.00 |2001  |0      |\n",
      "|67832 |CLARE   |MANAGER  |68319     |1991-06-09|2550.00|1001  |2750.00|\n",
      "|66928 |BLAZE   |MANAGER  |68319     |1991-05-01|2750.00|3001  |2957.00|\n",
      "|65646 |JONAS   |MANAGER  |68319     |1991-04-02|2957.00|2001  |0      |\n",
      "|68319 |KAYLING |PRESIDENT|          |1991-11-18|6000.00|1001  |0      |\n",
      "|65271 |WADE    |SALESMAN |66928     |1991-02-22|1350.00|3001  |1350.00|\n",
      "|66564 |MADDEN  |SALESMAN |66928     |1991-09-28|1350.00|3001  |1600.00|\n",
      "|68454 |TUCKER  |SALESMAN |66928     |1991-09-08|1600.00|3001  |1700.00|\n",
      "|64989 |ADELYN  |SALESMAN |66928     |1991-02-20|1700.00|3001  |0      |\n",
      "+------+--------+---------+----------+----------+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp_lead=df_emp.withColumn('lead',lead('salary',1,0).over(windowspec))\n",
    "df_emp_lead.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+-------+-------+\n",
      "|job_name |avg   |sum   |max    |min    |\n",
      "+---------+------+------+-------+-------+\n",
      "|ANALYST  |3100.0|6200.0|3100.00|3100.00|\n",
      "|CLERK    |1050.0|1050.0|1050.00|1050.00|\n",
      "|MANAGER  |2550.0|2550.0|2550.00|2550.00|\n",
      "|PRESIDENT|6000.0|6000.0|6000.00|6000.00|\n",
      "|SALESMAN |1350.0|2700.0|1350.00|1350.00|\n",
      "+---------+------+------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowsp=Window.partitionBy('job_name').orderBy('salary')\n",
    "\n",
    "df_emp_window_agg=df_emp.withColumn('row',row_number().over(windowsp))\\\n",
    "                        .withColumn('avg',avg(col('salary')).over(windowsp))\\\n",
    "                        .withColumn('sum',sum(col('salary')).over(windowsp))\\\n",
    "                        .withColumn('max',max(col('salary')).over(windowsp))\\\n",
    "                        .withColumn('min',min(col('salary')).over(windowsp))\\\n",
    "                        .where(col('row')==1).select('job_name','avg','sum','max','min')\n",
    "df_emp_window_agg.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "updatedDF = df.withColumn(\"dataset\",explode(col(\"dataset\"))).select(\"@context\",\"@id\",\"conformsTo\",\"dataset.@type\",\"dataset.accesslevel\",\"dataset.accrualPeriodicity\",\"dataset.bureauCode\",\"dataset.contactPoint.fn\",\"dataset.distribution.@type\",\"describedBy\")\n",
    "updatedDF.show()\n",
    "\n",
    "df_sel=df_emp_dep.select('job_name','salary').groupby('job_name').agg(avg('salary').alias('Salary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+----------+----------+-------+------+----+\n",
      "|emp_id|emp_name|job_name |manager_id|hire_date |salary |dep_id|when|\n",
      "+------+--------+---------+----------+----------+-------+------+----+\n",
      "|68319 |KAYLING |PRESIDENT|          |1991-11-18|6000.00|1001  |null|\n",
      "|66928 |BLAZE   |MANAGER  |68319     |1991-05-01|2750.00|3001  |null|\n",
      "|67832 |CLARE   |MANAGER  |68319     |1991-06-09|2550.00|1001  |null|\n",
      "|65646 |JONAS   |MANAGER  |68319     |1991-04-02|2957.00|2001  |null|\n",
      "|67858 |SCARLET |ANALYST  |65646     |1997-04-19|3100.00|2001  |null|\n",
      "|69062 |FRANK   |ANALYST  |65646     |1991-12-03|3100.00|2001  |null|\n",
      "|63679 |SANDRINE|CLERK    |69062     |1990-12-18|900.00 |2001  |clr |\n",
      "|64989 |ADELYN  |SALESMAN |66928     |1991-02-20|1700.00|3001  |sman|\n",
      "|65271 |WADE    |SALESMAN |66928     |1991-02-22|1350.00|3001  |sman|\n",
      "|66564 |MADDEN  |SALESMAN |66928     |1991-09-28|1350.00|3001  |sman|\n",
      "|68454 |TUCKER  |SALESMAN |66928     |1991-09-08|1600.00|3001  |sman|\n",
      "|68736 |ADNRES  |CLERK    |67858     |1997-05-23|1200.00|2001  |clr |\n",
      "|69000 |JULIUS  |CLERK    |66928     |1991-12-03|1050.00|3001  |clr |\n",
      "|69324 |MARKER  |CLERK    |67832     |1992-01-23|1400.00|1001  |clr |\n",
      "+------+--------+---------+----------+----------+-------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_when=df_emp.withColumn('when',when(df_emp.job_name=='CLERK','clr')\n",
    "                                 .when(df_emp.job_name=='SALESMAN','sman')    )\n",
    "df_when.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|job_name |lit_values|\n",
      "+---------+----------+\n",
      "|PRESIDENT|1         |\n",
      "|MANAGER  |1         |\n",
      "|MANAGER  |1         |\n",
      "|MANAGER  |1         |\n",
      "|ANALYST  |1         |\n",
      "|ANALYST  |1         |\n",
      "|CLERK    |1         |\n",
      "|SALESMAN |1         |\n",
      "|SALESMAN |1         |\n",
      "|SALESMAN |1         |\n",
      "|SALESMAN |1         |\n",
      "|CLERK    |1         |\n",
      "|CLERK    |1         |\n",
      "|CLERK    |1         |\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lit=df_emp.select(col(\"job_name\"),lit(1).alias(\"lit_values\"))\n",
    "df_lit.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|split_date  |\n",
      "+------------+\n",
      "|[1991-11-18]|\n",
      "|[1991-05-01]|\n",
      "|[1991-06-09]|\n",
      "|[1991-04-02]|\n",
      "|[1997-04-19]|\n",
      "|[1991-12-03]|\n",
      "|[1990-12-18]|\n",
      "|[1991-02-20]|\n",
      "|[1991-02-22]|\n",
      "|[1991-09-28]|\n",
      "|[1991-09-08]|\n",
      "|[1997-05-23]|\n",
      "|[1991-12-03]|\n",
      "|[1992-01-23]|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_split=df_emp.select(split(col('hire_date'),',').alias(\"split_date\"))\n",
    "df_split.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+\n",
      "|hire_date |year|month|day|\n",
      "+----------+----+-----+---+\n",
      "|1991-11-18|1991|11   |18 |\n",
      "|1991-05-01|1991|05   |01 |\n",
      "|1991-06-09|1991|06   |09 |\n",
      "|1991-04-02|1991|04   |02 |\n",
      "|1997-04-19|1997|04   |19 |\n",
      "|1991-12-03|1991|12   |03 |\n",
      "|1990-12-18|1990|12   |18 |\n",
      "|1991-02-20|1991|02   |20 |\n",
      "|1991-02-22|1991|02   |22 |\n",
      "|1991-09-28|1991|09   |28 |\n",
      "|1991-09-08|1991|09   |08 |\n",
      "|1997-05-23|1997|05   |23 |\n",
      "|1991-12-03|1991|12   |03 |\n",
      "|1992-01-23|1992|01   |23 |\n",
      "+----------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_substring=df_emp.select(col('hire_date'),substring('hire_date',1,4).alias(\"year\"),substring('hire_date',6,2).alias('month'),substring('hire_date',9,2).alias('day'))\n",
    "df_substring.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+----------+----------+-------+------+\n",
      "|emp_id|emp_name| job_name|manager_id| hire_date| salary|dep_id|\n",
      "+------+--------+---------+----------+----------+-------+------+\n",
      "| 69062|   FRANK|  ANALYST|     65646|1991-12-03|3100.00|  2001|\n",
      "| 67858| SCARLET|  ANALYST|     65646|1997-04-19|3100.00|  2001|\n",
      "| 68736|  ADNRES|    CLERK|     67858|1997-05-23|1200.00|  2001|\n",
      "| 69000|  JULIUS|    CLERK|     66928|1991-12-03|1050.00|  3001|\n",
      "| 63679|SANDRINE|    CLERK|     69062|1990-12-18| 900.00|  2001|\n",
      "| 69324|  MARKER|    CLERK|     67832|1992-01-23|1400.00|  1001|\n",
      "| 65646|   JONAS|  MANAGER|     68319|1991-04-02|2957.00|  2001|\n",
      "| 66928|   BLAZE|  MANAGER|     68319|1991-05-01|2750.00|  3001|\n",
      "| 67832|   CLARE|  MANAGER|     68319|1991-06-09|2550.00|  1001|\n",
      "| 68319| KAYLING|PRESIDENT|          |1991-11-18|6000.00|  1001|\n",
      "| 64989|  ADELYN| SALESMAN|     66928|1991-02-20|1700.00|  3001|\n",
      "| 65271|    WADE| SALESMAN|     66928|1991-02-22|1350.00|  3001|\n",
      "| 66564|  MADDEN| SALESMAN|     66928|1991-09-28|1350.00|  3001|\n",
      "| 68454|  TUCKER| SALESMAN|     66928|1991-09-08|1600.00|  3001|\n",
      "+------+--------+---------+----------+----------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sort=df_emp.sort(col('job_name'))\n",
    "df_sort.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ProjectPrordd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7792\\2345406738.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mProjectProAvg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mavg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mProjectPrordd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mProjectProAvg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ProjectPrordd' is not defined"
     ]
    }
   ],
   "source": [
    "def ProjectProAvg(x, y):\n",
    "    return (x+y)/2.0;\n",
    "avg = ProjectPrordd.reduce(ProjectProAvg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "df=spark.range(0,20)\n",
    "newdf=df.repartition(10)\n",
    "newcol=df.coalesce(2)\n",
    "print(newcol.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (DESKTOP-DDKQGNR executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11712\\1253779454.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbig_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0modds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0modds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\juypitor\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\juypitor\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1484\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1486\u001b[1;33m         \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1487\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\juypitor\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\juypitor\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (DESKTOP-DDKQGNR executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "conf=SparkConf().setAppName('test').setMaster('local[*]')\n",
    "sc=SparkContext(conf=conf)\n",
    "big_list=range(10000)\n",
    "rdd=sc.parallelize(big_list,2)\n",
    "odds=rdd.filter(lambda x: x%2!=0)\n",
    "odds.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------+\n",
      "| id|name|         skill|\n",
      "+---+----+--------------+\n",
      "|  1| abc|  [gcp, azure]|\n",
      "|  2| xyz|[python, java]|\n",
      "+---+----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1,'abc',['gcp','azure']),(2,'xyz',['python','java'])]\n",
    "schema=['id','name','skill']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------+------+\n",
      "| id|name|         skill|skills|\n",
      "+---+----+--------------+------+\n",
      "|  1| abc|  [gcp, azure]| false|\n",
      "|  2| xyz|[python, java]|  true|\n",
      "+---+----+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn('skills',array_contains(col('skill'),'java'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|emp_name| job_name|country|\n",
      "+--------+---------+-------+\n",
      "| KAYLING|PRESIDENT|  india|\n",
      "|   BLAZE|  MANAGER|  india|\n",
      "|   CLARE|  MANAGER|  india|\n",
      "|   JONAS|  MANAGER|  india|\n",
      "| SCARLET|  ANALYST|  india|\n",
      "|   FRANK|  ANALYST|  india|\n",
      "|SANDRINE|    CLERK|  india|\n",
      "|  ADELYN| SALESMAN|  india|\n",
      "|    WADE| SALESMAN|  india|\n",
      "|  MADDEN| SALESMAN|  india|\n",
      "|  TUCKER| SALESMAN|  india|\n",
      "|  ADNRES|    CLERK|  india|\n",
      "|  JULIUS|    CLERK|  india|\n",
      "|  MARKER|    CLERK|  india|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lit=df_emp.select(col(\"emp_name\"),col(\"job_name\"),lit(\"india\").alias(\"country\"))\n",
    "df_lit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+\n",
      "|emp_name|job_name|country|\n",
      "+--------+--------+-------+\n",
      "|SANDRINE|   CLERK|  india|\n",
      "|  ADNRES|   CLERK|  india|\n",
      "|  JULIUS|   CLERK|  india|\n",
      "|  MARKER|   CLERK|  india|\n",
      "+--------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter=df_lit.filter((col(\"job_name\")==\"CLERK\") | (col(\"emp_name\")==\"MARKER\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o36.csv.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"gs\"\r\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\r\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\r\n\tat scala.collection.immutable.List.map(List.scala:293)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m current_date \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 19\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(csv_file_path)\n\u001b[0;32m     22\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mD:\\jupitor\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[1;32mD:\\jupitor\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\jupitor\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mD:\\jupitor\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o36.csv.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"gs\"\r\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\r\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\r\n\tat scala.collection.immutable.List.map(List.scala:293)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "\n",
    " \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GCS Read Current Date CSV\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/path/to/your/service-account-key.json\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "bucket_name = \"your-gcs-bucket-name\"\n",
    "folder_path = \"path/to/csv/files\"\n",
    "\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "csv_file_path = f\"gs://{bucket_name}/{folder_path}/{current_date}.csv\"\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(csv_file_path)\n",
    "\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1468053357.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    val spark = SparkSession.builder().appName(\"GCS Read Current Date CSV\").config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\").config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\").config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/path/to/your/service-account-key.json\").getOrCreate()\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import java.time.LocalDate\n",
    "import java.time.format.DateTimeFormatter\n",
    "\n",
    "val spark = SparkSession.builder().appName(\"GCS Read Current Date CSV\").config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\").config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\").config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/path/to/your/service-account-key.json\").getOrCreate()\n",
    "\n",
    "val bucketName = \"your-gcs-bucket-name\"\n",
    "val folderPath = \"path/to/csv/files\"\n",
    "\n",
    "val currentDate = LocalDate.now().format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd\"))\n",
    "\n",
    "val csvFilePath = s\"gs://$bucketName/$folderPath/$currentDate.csv\"\n",
    "\n",
    "val df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(csvFilePath)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o76.csv.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:180)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:133)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:68)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:539)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:405)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#csv_file_path = f\"gs://{bucket_name}/{folder_path}/{current_date}.csv\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(folder_path)\n\u001b[0;32m     16\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mD:\\jupitor\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[1;32mD:\\jupitor\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\jupitor\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mD:\\jupitor\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o76.csv.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:180)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:133)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:68)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:539)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:405)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Python Spark SQL basic example\").getOrCreate()\n",
    "\n",
    "bucket_name = \"your-gcs-bucket-name\"\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\Kiran Adhav\\\\OneDrive\\\\Desktop\\\\kiran\\\\\"\n",
    "\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "csv_file_path = f\"gs://{bucket_name}/{folder_path}/{current_date}.csv\"\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(csv_file_path)\n",
    "\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set up GCS and BigQuery client\n",
    "storage_client = storage.Client()\n",
    "bigquery_client = bigquery.Client()\n",
    "\n",
    "# Define your GCS bucket and BigQuery dataset/table names\n",
    "gcs_bucket_name = 'your-gcs-bucket-name'\n",
    "gcs_folder = 'path/to/your/csv/files'\n",
    "bigquery_dataset_name = 'your_dataset_name'\n",
    "bigquery_table_name_base = 'your_table_name_base'  # Base name for the table, e.g., 'my_table'\n",
    "\n",
    "# Get the current date in YYYY_MM_DD format for date-based naming\n",
    "current_date = datetime.now().strftime('%Y_%m_%d')\n",
    "\n",
    "# Define BigQuery dataset and table names with the current date\n",
    "bigquery_table_name = f\"{bigquery_table_name_base}_{current_date}\"\n",
    "\n",
    "# Define the full table ID in BigQuery (project.dataset.table)\n",
    "table_id = f\"{bigquery_client.project}.{bigquery_dataset_name}.{bigquery_table_name}\"\n",
    "\n",
    "# Create a BigQuery dataset if it does not exist\n",
    "dataset_id = f\"{bigquery_client.project}.{bigquery_dataset_name}\"\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = \"US\"  # Set your location\n",
    "bigquery_client.create_dataset(dataset, exists_ok=True)\n",
    "\n",
    "# List all files in the GCS folder\n",
    "bucket = storage_client.get_bucket(gcs_bucket_name)\n",
    "blobs = bucket.list_blobs(prefix=gcs_folder)\n",
    "\n",
    "# Process each CSV file\n",
    "for blob in blobs:\n",
    "    if blob.name.endswith('.csv'):\n",
    "        print(f\"Processing file: {blob.name}\")\n",
    "\n",
    "        # Download the CSV file from GCS as a pandas DataFrame\n",
    "        blob_data = blob.download_as_text()\n",
    "        df = pd.read_csv(pd.compat.StringIO(blob_data))\n",
    "\n",
    "        # Load the DataFrame into BigQuery\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "            source_format=bigquery.SourceFormat.CSV,\n",
    "            autodetect=True,\n",
    "        )\n",
    "\n",
    "        load_job = bigquery_client.load_table_from_dataframe(\n",
    "            df, table_id, job_config=job_config\n",
    "        )\n",
    "        load_job.result()  # Wait for the job to complete\n",
    "\n",
    "        print(f\"Loaded data into BigQuery table {table_id}\")\n",
    "\n",
    "print(\"All CSV files have been processed and loaded into BigQuery.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
